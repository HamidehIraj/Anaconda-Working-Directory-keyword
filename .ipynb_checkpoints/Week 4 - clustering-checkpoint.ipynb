{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "## Suppressing Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Loading Libraries\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(213, 16)\n"
     ]
    }
   ],
   "source": [
    "## Load the dataset\n",
    "gap= pd.read_csv(\"gapminder.csv\")\n",
    "print(gap.shape)\n",
    "## Removing NAs\n",
    "# gap= gap_tree.dropna()\n",
    "\n",
    "# #gap.dtypes\n",
    "# #gap.describe()\n",
    "# #gap.suicideper100th.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164, 16)\n"
     ]
    }
   ],
   "source": [
    "## Preparing Data\n",
    "\n",
    "gap['urbanrate']        = pd.to_numeric(gap['urbanrate']       ,errors='coerce')\n",
    "gap['internetuserate']  = pd.to_numeric(gap['internetuserate'] ,errors='coerce')\n",
    "gap['femaleemployrate'] = pd.to_numeric(gap['femaleemployrate'],errors='coerce')\n",
    "gap['incomeperperson']  = pd.to_numeric(gap['incomeperperson'] ,errors='coerce')\n",
    "\n",
    "gap = gap[(gap['urbanrate']>0) & (gap['internetuserate']>0) & (gap['femaleemployrate']>0) & (gap['incomeperperson']>0)]\n",
    "\n",
    "#gap = gap[['country','urbanrate','internetuserate','femaleemployrate','incomeperperson']]\n",
    "\n",
    "print(gap.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standardize clustering variables to have mean=0 and sd=1\n",
    "gap_backup=gap.copy()\n",
    "gap['urbanrate']       =preprocessing.scale(gap['urbanrate'].astype('float64'))\n",
    "gap['internetuserate'] =preprocessing.scale(gap['internetuserate'].astype('float64'))\n",
    "gap['femaleemployrate']=preprocessing.scale(gap['femaleemployrate'].astype('float64'))\n",
    "gap['incomeperperson'] =preprocessing.scale(gap['incomeperperson'].astype('float64'))\n",
    "\n",
    "# gap_quant= gap[['country','urbanrate','internetuserate','femaleemployrate','incomeperperson']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1581825ff18e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclus_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mclusassign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclus_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     meandist.append(sum(np.min(cdist(clus_train, model.cluster_centers_, 'euclidean'), axis=1)) \n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    783\u001b[0m         \"\"\"\n\u001b[0;32m    784\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_check_fit_data\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    753\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_fit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;34m\"\"\"Verify that the number of samples given is larger than k\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             raise ValueError(\"n_samples=%d should be >= n_clusters=%d\" % (\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m         \u001b[1;31m# make sure we actually converted to numeric:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"O\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: "
     ]
    }
   ],
   "source": [
    "# split data into train and test sets\n",
    "clus_train, clus_test = train_test_split(gap, test_size=.3, random_state=123) \n",
    "                                         \n",
    "# k-means cluster analysis for 1-9 clusters                                                           \n",
    "from scipy.spatial.distance import cdist\n",
    "clusters=range(1,10)\n",
    "meandist=[]\n",
    "\n",
    "for k in clusters:\n",
    "    model=KMeans(n_clusters=k)\n",
    "    model.fit(clus_train)\n",
    "    clusassign=model.predict(clus_train)\n",
    "    meandist.append(sum(np.min(cdist(clus_train, model.cluster_centers_, 'euclidean'), axis=1)) \n",
    "    / clus_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot average distance from observations from the cluster centroid\n",
    "to use the Elbow Method to identify number of clusters to choose\n",
    "\"\"\"\n",
    "\n",
    "plt.plot(clusters, meandist)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average distance')\n",
    "plt.title('Selecting k with the Elbow Method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Interpret 3 cluster solution\n",
    "model3=KMeans(n_clusters=3)\n",
    "model3.fit(clus_train)\n",
    "clusassign=model3.predict(clus_train)\n",
    "# plot clusters\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca_2 = PCA(2)\n",
    "plot_columns = pca_2.fit_transform(clus_train)\n",
    "plt.scatter(x=plot_columns[:,0], y=plot_columns[:,1], c=model3.labels_,)\n",
    "plt.xlabel('Canonical variable 1')\n",
    "plt.ylabel('Canonical variable 2')\n",
    "plt.title('Scatterplot of Canonical Variables for 3 Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEGIN multiple steps to merge cluster assignment with clustering variables to examine\n",
    "# cluster variable means by cluster\n",
    "\n",
    "\n",
    "# create a unique identifier variable from the index for the \n",
    "# cluster training data to merge with the cluster assignment variable\n",
    "clus_train.reset_index(level=0, inplace=True)\n",
    "\n",
    "# create a list that has the new index variable\n",
    "cluslist=list(clus_train['index'])\n",
    "print(cluslist)\n",
    "\n",
    "# create a list of cluster assignments\n",
    "labels=list(model3.labels_)\n",
    "\n",
    "# combine index variable list with cluster assignment list into a dictionary\n",
    "# newlist=dict(zip(cluslist, labels))\n",
    "newlist=dict(zip(cluslist, labels))\n",
    "# newlist\n",
    "\n",
    "# convert newlist dictionary to a dataframe\n",
    "newclus=DataFrame.from_dict(newlist, orient='index')\n",
    "newclus\n",
    "\n",
    "# rename the cluster assignment column\n",
    "newclus.columns = ['cluster']\n",
    "\n",
    "# now do the same for the cluster assignment variable\n",
    "# create a unique identifier variable from the index for the \n",
    "# cluster assignment dataframe \n",
    "# to merge with cluster training data\n",
    "newclus.reset_index(level=0, inplace=True)\n",
    "\n",
    "# merge the cluster assignment dataframe with the cluster training variable dataframe\n",
    "# by the index variable\n",
    "merged_train=pd.merge(clus_train, newclus, on='index')\n",
    "merged_train.head(n=100)\n",
    "\n",
    "# cluster frequencies\n",
    "print('cluster frequencies')\n",
    "merged_train.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "END multiple steps to merge cluster assignment with clustering variables to examine\n",
    "cluster variable means by cluster\n",
    "\"\"\"\n",
    "merged_train_report = merged_train[['urbanrate','internetuserate','femaleemployrate','incomeperperson','cluster']]\n",
    "#merged_train_report.head()\n",
    "\n",
    "# FINALLY calculate clustering variable means by cluster\n",
    "clustergrp = merged_train_report.groupby('cluster').mean()\n",
    "print (\"Clustering variable means by cluster\")\n",
    "clustergrp.head()\n",
    "\n",
    "# merged_train_0 = merged_train[(merged_train['cluster']==0) ]\n",
    "# merged_train_1 = merged_train[(merged_train['cluster']==1) ]\n",
    "# merged_train_2 = merged_train[(merged_train['cluster']==2) ]\n",
    "\n",
    "# print(merged_train_0)\n",
    "# print(merged_train_1)\n",
    "# print(merged_train_2)\n",
    "# merged_train_0.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # validate clusters in training data by examining cluster differences in GPA using ANOVA\n",
    "# # first have to merge GPA with clustering variables and cluster assignment data \n",
    "# gpa_data=data_clean['GPA1']\n",
    "# # split GPA data into train and test sets\n",
    "# gpa_train, gpa_test = train_test_split(gpa_data, test_size=.3, random_state=123)\n",
    "# gpa_train1=pd.DataFrame(gpa_train)\n",
    "# gpa_train1.reset_index(level=0, inplace=True)\n",
    "# merged_train_all=pd.merge(gpa_train1, merged_train, on='index')\n",
    "# sub1 = merged_train_all[['GPA1', 'cluster']].dropna()\n",
    "\n",
    "# import statsmodels.formula.api as smf\n",
    "# import statsmodels.stats.multicomp as multi \n",
    "\n",
    "# gpamod = smf.ols(formula='GPA1 ~ C(cluster)', data=sub1).fit()\n",
    "# print (gpamod.summary())\n",
    "\n",
    "# print ('means for GPA by cluster')\n",
    "# m1= sub1.groupby('cluster').mean()\n",
    "# print (m1)\n",
    "\n",
    "# print ('standard deviations for GPA by cluster')\n",
    "# m2= sub1.groupby('cluster').std()\n",
    "# print (m2)\n",
    "\n",
    "# mc1 = multi.MultiComparison(sub1['GPA1'], sub1['cluster'])\n",
    "# res1 = mc1.tukeyhsd()\n",
    "# print(res1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "VALIDATION\n",
    "BEGIN multiple steps to merge cluster assignment with clustering variables to examine\n",
    "cluster variable means by cluster in test data set\n",
    "\"\"\"\n",
    "# create a variable out of the index for the cluster training dataframe to merge on\n",
    "clus_test.reset_index(level=0, inplace=True)\n",
    "# create a list that has the new index variable\n",
    "cluslistval=list(clus_test['index'])\n",
    "# create a list of cluster assignments\n",
    "labelsval=list(clusassignval)\n",
    "# combine index variable list with labels list into a dictionary\n",
    "newlistval=dict(zip(cluslistval, clusassignval))\n",
    "newlistval\n",
    "# convert newlist dictionary to a dataframe\n",
    "newclusval=DataFrame.from_dict(newlistval, orient='index')\n",
    "newclusval\n",
    "# rename the cluster assignment column\n",
    "newclusval.columns = ['cluster']\n",
    "# create a variable out of the index for the cluster assignment dataframe to merge on\n",
    "newclusval.reset_index(level=0, inplace=True)\n",
    "# merge the cluster assignment dataframe with the cluster training variable dataframe\n",
    "# by the index variable\n",
    "merged_test=pd.merge(clus_test, newclusval, on='index')\n",
    "# cluster frequencies\n",
    "merged_test.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "END multiple steps to merge cluster assignment with clustering variables to examine\n",
    "cluster variable means by cluster\n",
    "\"\"\"\n",
    "\n",
    "# calculate test data clustering variable means by cluster\n",
    "clustergrpval = merged_test.groupby('cluster').mean()\n",
    "print (\"Test data clustering variable means by cluster\")\n",
    "print(clustergrpval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
